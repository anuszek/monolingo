<!DOCTYPE html>
<html lang="pl">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Monolingo â€” Voice Agent</title>
    <style>
      body {
        font-family: system-ui, sans-serif;
        display: flex;
        align-items: center;
        justify-content: center;
        height: 100vh;
        margin: 0;
        background: #f6f9fc;
      }
      .card {
        background: #fff;
        padding: 24px;
        border-radius: 12px;
        box-shadow: 0 6px 18px rgba(0, 0, 0, 0.08);
        width: 360px;
        text-align: center;
      }
      button {
        background: #2563eb;
        color: #fff;
        border: none;
        padding: 12px 18px;
        border-radius: 8px;
        cursor: pointer;
      }
      button.recording {
        background: #ef4444;
      }
      #transcript {
        margin-top: 12px;
        min-height: 40px;
        color: #111;
      }
    </style>
  </head>
  <body>
    <div class="card">
      <h2>Rozmowa z AI</h2>
      <p>
        NaciÅ›nij mikrofon i mÃ³w. Wynik zostanie wysÅ‚any do agenta, a odpowiedÅº
        odtworzona.
      </p>
      <button id="micBtn" title="NaciÅ›nij, aby mÃ³wiÄ‡">ðŸŽ¤ Start</button>
      <label for="practiceLang">JÄ™zyk Ä‡wiczeÅ„:</label>
      <select id="practiceLang">
        <option value="en-US">English (en-US)</option>
        <option value="pl-PL">Polski (pl-PL)</option>
      </select>
      <div style="margin-top: 12px; text-align: left">
        <label for="imageInput">Wstaw zdjÄ™cie (OCR):</label>
        <input id="imageInput" type="file" accept="image/*" />
        <label
          ><input id="forwardToAgent" type="checkbox" /> WyÅ›lij rozpoznany tekst
          do agenta</label
        >
        <button id="uploadBtn">PrzeÅ›lij obraz</button>
      </div>
      <div id="transcript"></div>
    </div>

    <script>
      const micBtn = document.getElementById("micBtn");
      const transcriptEl = document.getElementById("transcript");
      let recognizing = false;
      let recognition;
      let selectedVoice = null;

      if (
        !("webkitSpeechRecognition" in window) &&
        !("SpeechRecognition" in window)
      ) {
        transcriptEl.textContent =
          "Twoja przeglÄ…darka nie wspiera Web Speech API. UÅ¼yj Chrome lub Edge.";
        micBtn.disabled = true;
      } else {
        const SpeechRecognition =
          window.SpeechRecognition || window.webkitSpeechRecognition;
        recognition = new SpeechRecognition();
        recognition.lang = "pl-PL";
        recognition.interimResults = false;
        recognition.maxAlternatives = 1;

        recognition.addEventListener("result", async (e) => {
          const text = Array.from(e.results)
            .map((r) => r[0].transcript)
            .join("");
          transcriptEl.textContent = "Rozpoznano: " + text;

          // wyÅ›lij do agenta (TTS po stronie serwera)
          try {
            // wysyÅ‚amy tekst do /api/agent-tts i otrzymamy audio (mp3)
            const practiceLang =
              document.getElementById("practiceLang").value || "en-US";
            const resp = await fetch("/XD/api/agent-tts", {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ message: text, lang: practiceLang }),
            });

            if (!resp.ok) {
              const errText = await resp.text();
              transcriptEl.textContent += "\nBÅ‚Ä…d TTS: " + errText;
              return;
            }

            const blob = await resp.blob();
            const url = URL.createObjectURL(blob);
            const audio = new Audio(url);
            audio.autoplay = true;
            transcriptEl.textContent += "\nOdtwarzam odpowiedÅº...";

            audio.addEventListener("ended", () => {
              URL.revokeObjectURL(url);
              // wznowienie rozpoznawania, jeÅ›li uÅ¼ytkownik byÅ‚ w trybie nagrywania
              if (recognizing) {
                try {
                  recognition.start();
                  transcriptEl.textContent = "SÅ‚ucham...";
                } catch (e) {}
              }
            });
          } catch (err) {
            transcriptEl.textContent += "\nBÅ‚Ä…d poÅ‚Ä…czenia: " + err.message;
          }
        });

        // przygotuj listÄ™ gÅ‚osÃ³w i wybierz polski gÅ‚os o nazwie zawierajÄ…cej 'Iwona' jeÅ›li dostÄ™pny
        function prepareVoices() {
          const voices = speechSynthesis.getVoices() || [];
          // preferuj gÅ‚os zawierajÄ…cy 'Iwona', inaczej polskie gÅ‚osy
          selectedVoice =
            voices.find((v) => /iwona/i.test(v.name)) ||
            voices.find(
              (v) =>
                /pol(ish|ski)/i.test(v.lang) || /pol(ish|ski)/i.test(v.name)
            );
        }

        // niektÃ³re przeglÄ…darki Å‚adujÄ… gÅ‚osy asynchronicznie
        prepareVoices();
        if ("onvoiceschanged" in speechSynthesis) {
          speechSynthesis.onvoiceschanged = prepareVoices;
        }

        recognition.addEventListener("end", () => {
          if (recognizing) {
            recognizing = false;
            micBtn.classList.remove("recording");
            micBtn.textContent = "ðŸŽ¤ Start";
          }
        });

        recognition.addEventListener("error", (e) => {
          transcriptEl.textContent = "BÅ‚Ä…d rozpoznawania: " + e.error;
        });
      }

      micBtn.addEventListener("click", () => {
        if (!recognition) return;
        if (!recognizing) {
          recognition.start();
          recognizing = true;
          micBtn.classList.add("recording");
          micBtn.textContent = "â¹ Stop";
          transcriptEl.textContent = "SÅ‚ucham...";
        } else {
          recognition.stop();
          recognizing = false;
          micBtn.classList.remove("recording");
          micBtn.textContent = "ðŸŽ¤ Start";
        }
      });

      // OCR upload
      const uploadBtn = document.getElementById("uploadBtn");
      uploadBtn.addEventListener("click", async () => {
        const fileEl = document.getElementById("imageInput");
        if (!fileEl.files || fileEl.files.length === 0)
          return alert("Wybierz obraz");
        const file = fileEl.files[0];
        const form = new FormData();
        form.append("image", file);
        // opcjonalne forwardowanie do agenta
        const forward = document.getElementById("forwardToAgent").checked;
        if (forward) form.append("sendToAgent", "true");

        transcriptEl.textContent = "WysyÅ‚am obraz do OCR...";
        try {
          const resp = await fetch("/XD/api/ocr", {
            method: "POST",
            body: form,
          });
          if (!resp.ok) {
            const t = await resp.text();
            transcriptEl.textContent = "BÅ‚Ä…d OCR: " + t;
            return;
          }
          const data = await resp.json();
          transcriptEl.textContent =
            "Rozpoznany tekst:\n" + (data.text || "Brak");
          if (data.reply) {
            transcriptEl.textContent += "\nAI: " + data.reply;
            // jeÅ›li serwer zwrÃ³ciÅ‚ reply i chcemy odtworzyÄ‡ audio, wywoÅ‚aj /api/agent-tts
            const practiceLang =
              document.getElementById("practiceLang").value || "en-US";
            const ttsResp = await fetch("/XD/api/agent-tts", {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ message: data.text, lang: practiceLang }),
            });
            if (ttsResp.ok) {
              const blob = await ttsResp.blob();
              const url = URL.createObjectURL(blob);
              const audio = new Audio(url);
              audio.play();
              audio.addEventListener("ended", () => URL.revokeObjectURL(url));
            }
          }
        } catch (err) {
          transcriptEl.textContent = "BÅ‚Ä…d: " + err.message;
        }
      });
    </script>
  </body>
</html>
